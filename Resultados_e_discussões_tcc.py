# -*- coding: utf-8 -*-
"""Resultados e Discussões TCC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m8XS_suKstcm_6_duR3j7KyzYTiu8xLU

# **DESCRIÇÃO DOS CÓDIGOS PARA DESENVOLVIEMNTOS DAS AVALIAÇÕES DOS DEPOIMENTOS**

# Bibliotecas Utilizadas
"""

!pip install python-docx

import re
import nltk
import spacy
import spacy.cli
import subprocess
import pandas as pd
import string
import matplotlib.pyplot as plt

# Baixar recursos necessários do NLTK
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')

from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag
from nltk.probability import FreqDist
from nltk.corpus import stopwords
from docx import Document
from collections import Counter
from string import punctuation
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.tokenize.treebank import TreebankWordDetokenizer
from nltk.translate.bleu_score import sentence_bleu

"""# Frequencia das palavras chave (Gráfico de pareto com porcentagem acumulada)_altruista"""

import matplotlib.pyplot as plt
from docx import Document
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from string import punctuation
from sklearn.feature_extraction.text import TfidfVectorizer

# Função para carregar o conteúdo de um arquivo .docx
def carregar_docx(caminho_arquivo):
    doc = Document(caminho_arquivo)
    texto = []
    for paragraph in doc.paragraphs:
        texto.append(paragraph.text)
    return '\n'.join(texto)

# Função para limpar o texto removendo stopwords, pontuação e palavras específicas
def limpar_texto(texto):
    # Tokenizar o texto em palavras
    palavras = word_tokenize(texto)

    # Obter lista de stopwords em português
    stopwords_portugues = set(stopwords.words('portuguese'))

    # Remover stopwords, pontuação e palavras específicas
    palavras_limpas = [palavra.lower() for palavra in palavras
                       if palavra.lower() not in stopwords_portugues
                       and palavra.lower() not in punctuation
                       and palavra.lower() != 'histórias'
                       and palavra.lower() != 'sobreviventes'
                       and palavra.lower() != 'suicídio'
                       and palavra.lower() != 'vol']

    # Juntar as palavras novamente em um texto limpo
    texto_limpo = ' '.join(palavras_limpas)

    return texto_limpo

# Carregar o conteúdo do arquivo .docx
caminho_arquivo_docx = 'altruista.docx'
texto_docx = carregar_docx(caminho_arquivo_docx)

# Limpar o texto removendo stopwords, pontuação e palavras específicas
texto_limpo = limpar_texto(texto_docx)

# Inicializar e ajustar o modelo TF-IDF
tfidf_modelo = TfidfVectorizer()
tfidf_modelo.fit([texto_limpo])

# Obter os termos (palavras) e seus respectivos TF-IDF
termos = tfidf_modelo.get_feature_names_out()
tfidf_valores = tfidf_modelo.transform([texto_limpo]).toarray()[0]

# Definir um limite para o TF-IDF
limite_tfidf = 0.1

# Selecionar palavras com TF-IDF acima do limite
palavras_importantes_tfidf = [(termo, tfidf) for termo, tfidf in zip(termos, tfidf_valores) if tfidf > limite_tfidf]

# Separar as palavras e seus valores de TF-IDF
palavras_tfidf, valores_tfidf = zip(*palavras_importantes_tfidf) if palavras_importantes_tfidf else ([], [])

# Plotar o gráfico de linha das palavras consideradas importantes pelo TF-IDF
plt.figure(figsize=(10, 6))
plt.plot(palavras_tfidf, valores_tfidf, marker='o', linestyle='-', color='b')
plt.title('altruista.docx Palavras Mais Importantes (TF-IDF > 0.1)', fontsize=16, fontweight='bold')
plt.xlabel('Palavra', fontsize=16, fontweight='bold')
plt.ylabel('TF-IDF', fontsize=16, fontweight='bold')
plt.xticks(rotation=90, fontsize=18, fontweight='bold', ha='right')
plt.grid(True)
plt.tight_layout()

# Adicionar rótulos com os números dos pontos
for i, txt in enumerate(valores_tfidf):
    plt.text(palavras_tfidf[i], txt, f'{txt:.2f}', fontsize=12, ha='center', va='bottom')

plt.show()

import matplotlib.pyplot as plt
from collections import Counter
from docx import Document
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from string import punctuation
import nltk
import re
import os

# Certifique-se de baixar os recursos necessários do NLTK
nltk.download('punkt')
nltk.download('stopwords')

# Função para carregar SentiLex-PT02
def carregar_sentilex():
    sentilex = {}
    caminho_sentilex = 'SentiLex-lem-PT02.txt'

    if not os.path.exists(caminho_sentilex):
        raise FileNotFoundError(f"O arquivo {caminho_sentilex} não foi encontrado.")

    with open(caminho_sentilex, 'r', encoding='utf-8') as f:
        for line in f:
            parts = line.strip().split(';')
            termo = parts[0].split('.')[0]  # Extrair a palavra
            polaridades = [int(re.search(r'POL:N\d=(-?\d)', part).group(1)) for part in parts if 'POL:N' in part]
            if polaridades:
                sentilex[termo] = polaridades[0]  # Usar a primeira polaridade encontrada
    return sentilex

# Função para carregar o conteúdo de um arquivo .docx
def carregar_docx(caminho_arquivo):
    doc = Document(caminho_arquivo)
    texto = []
    for paragraph in doc.paragraphs:
        texto.append(paragraph.text)
    return '\n'.join(texto)

# Função para limpar o texto removendo stopwords, pontuação, aspas, pontos, traços, travessões e palavras específicas
def limpar_texto(texto):
    # Remover pontos, traços, travessões e aspas
    texto = re.sub(r'[.–-“”]', ' ', texto)

    # Tokenizar o texto em palavras
    palavras = word_tokenize(texto)

    # Obter lista de stopwords em português
    stopwords_portugues = set(stopwords.words('portuguese'))

    # Remover stopwords, pontuação e palavras específicas
    palavras_limpas = [palavra.lower() for palavra in palavras
                       if palavra.lower() not in stopwords_portugues
                       and palavra not in punctuation
                       and palavra.lower() != 'histórias'
                       and palavra.lower() != 'sobreviventes'
                       and palavra.lower() != 'suicídio'
                       and palavra.lower() != 'vol']

    return palavras_limpas


# Carregar o conteúdo do arquivo .docx
caminho_arquivo_docx = 'altruista.docx'
texto_docx = carregar_docx(caminho_arquivo_docx)

# Limpar o texto removendo stopwords, pontuação, aspas, pontos, traços, travessões e palavras específicas
palavras_limpas = limpar_texto(texto_docx)

# Carregar o dicionário SentiLex
sentilex = carregar_sentilex()

# Filtrar palavras que estão no SentiLex e calcular suas frequências
palavras_sentimentos = [palavra for palavra in palavras_limpas if palavra in sentilex]
frequencia_palavras_sentimentos = Counter(palavras_sentimentos)

# Selecionar as palavras com frequência maior que 2
top_palavras_filtradas = [(palavra, freq) for palavra, freq in frequencia_palavras_sentimentos.items() if freq > 3]

# Separar as palavras e suas frequências filtradas
palavras_filtradas, frequencias_filtradas = zip(*top_palavras_filtradas)

# Plotar o gráfico de barras das palavras de sentimentos filtradas
plt.figure(figsize=(14, 8))  # Ajustar o tamanho da figura conforme necessário
bars = plt.bar(palavras_filtradas, frequencias_filtradas, color='b', width=0.5)  # Ajustar a largura das barras conforme necessário
plt.title('Gráfico de Barras das Palavras de Sentimentos com Frequência Maior que 2', fontsize=16, fontweight='bold')
plt.xlabel('Palavra', fontsize=16, fontweight='bold')
plt.ylabel('Frequência', fontsize=16, fontweight='bold')
plt.xticks(rotation=90, fontsize=12, fontweight='bold', ha='right')  # Ajustar o ângulo e o tamanho da fonte no eixo x
# Adicionar valor de frequência em cima de cada barra
for bar, freq in zip(bars, frequencias_filtradas):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), str(freq), ha='center', va='bottom', fontsize=10, fontweight='bold')

"""# Frequencia das palavras chave (Gráfico de pareto com porcentagem acumulada)_egoista"""

import matplotlib.pyplot as plt
from docx import Document
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from string import punctuation
from sklearn.feature_extraction.text import TfidfVectorizer

# Função para carregar o conteúdo de um arquivo .docx
def carregar_docx(caminho_arquivo):
    doc = Document(caminho_arquivo)
    texto = []
    for paragraph in doc.paragraphs:
        texto.append(paragraph.text)
    return '\n'.join(texto)

# Função para limpar o texto removendo stopwords, pontuação e palavras específicas
def limpar_texto(texto):
    # Tokenizar o texto em palavras
    palavras = word_tokenize(texto)

    # Obter lista de stopwords em português
    stopwords_portugues = set(stopwords.words('portuguese'))

    # Remover stopwords, pontuação e palavras específicas
    palavras_limpas = [palavra.lower() for palavra in palavras
                       if palavra.lower() not in stopwords_portugues
                       and palavra.lower() not in punctuation
                       and palavra.lower() != 'histórias'
                       and palavra.lower() != 'sobreviventes'
                       and palavra.lower() != 'suicídio'
                       and palavra.lower() != 'vol']

    # Juntar as palavras novamente em um texto limpo
    texto_limpo = ' '.join(palavras_limpas)

    return texto_limpo

# Carregar o conteúdo do arquivo .docx
caminho_arquivo_docx = 'egoista.docx'
texto_docx = carregar_docx(caminho_arquivo_docx)

# Limpar o texto removendo stopwords, pontuação e palavras específicas
texto_limpo = limpar_texto(texto_docx)

# Inicializar e ajustar o modelo TF-IDF
tfidf_modelo = TfidfVectorizer()
tfidf_modelo.fit([texto_limpo])

# Obter os termos (palavras) e seus respectivos TF-IDF
termos = tfidf_modelo.get_feature_names_out()
tfidf_valores = tfidf_modelo.transform([texto_limpo]).toarray()[0]

# Definir um limite para o TF-IDF
limite_tfidf = 0.1

# Selecionar palavras com TF-IDF acima do limite
palavras_importantes_tfidf = [(termo, tfidf) for termo, tfidf in zip(termos, tfidf_valores) if tfidf > limite_tfidf]

# Separar as palavras e seus valores de TF-IDF
palavras_tfidf, valores_tfidf = zip(*palavras_importantes_tfidf) if palavras_importantes_tfidf else ([], [])

# Plotar o gráfico de linha das palavras consideradas importantes pelo TF-IDF
plt.figure(figsize=(10, 6))
plt.plot(palavras_tfidf, valores_tfidf, marker='o', linestyle='-', color='b')
plt.title('egoista.docx Palavras Mais Importantes (TF-IDF > 0.1)', fontsize=16, fontweight='bold')
plt.xlabel('Palavra', fontsize=16, fontweight='bold')
plt.ylabel('TF-IDF', fontsize=16, fontweight='bold')
plt.xticks(rotation=45, fontsize=14, fontweight='bold', ha='right')
plt.grid(True)
plt.tight_layout()

# Adicionar rótulos com os números dos pontos
for i, txt in enumerate(valores_tfidf):
    plt.text(palavras_tfidf[i], txt, f'{txt:.2f}', fontsize=12, ha='center', va='bottom')

plt.show()



import matplotlib.pyplot as plt
from collections import Counter
from docx import Document
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from string import punctuation
import nltk
import re
import os

# Certifique-se de baixar os recursos necessários do NLTK
nltk.download('punkt')
nltk.download('stopwords')

# Função para carregar SentiLex-PT02
def carregar_sentilex():
    sentilex = {}
    caminho_sentilex = 'SentiLex-lem-PT02.txt'

    if not os.path.exists(caminho_sentilex):
        raise FileNotFoundError(f"O arquivo {caminho_sentilex} não foi encontrado.")

    with open(caminho_sentilex, 'r', encoding='utf-8') as f:
        for line in f:
            parts = line.strip().split(';')
            termo = parts[0].split('.')[0]  # Extrair a palavra
            polaridades = [int(re.search(r'POL:N\d=(-?\d)', part).group(1)) for part in parts if 'POL:N' in part]
            if polaridades:
                sentilex[termo] = polaridades[0]  # Usar a primeira polaridade encontrada
    return sentilex

# Função para carregar o conteúdo de um arquivo .docx
def carregar_docx(caminho_arquivo):
    doc = Document(caminho_arquivo)
    texto = []
    for paragraph in doc.paragraphs:
        texto.append(paragraph.text)
    return '\n'.join(texto)

# Função para limpar o texto removendo stopwords, pontuação, aspas, pontos, traços, travessões e palavras específicas
def limpar_texto(texto):
    # Remover pontos, traços, travessões e aspas
    texto = re.sub(r'[.–-“”]', ' ', texto)

    # Tokenizar o texto em palavras
    palavras = word_tokenize(texto)

    # Obter lista de stopwords em português
    stopwords_portugues = set(stopwords.words('portuguese'))

    # Remover stopwords, pontuação e palavras específicas
    palavras_limpas = [palavra.lower() for palavra in palavras
                       if palavra.lower() not in stopwords_portugues
                       and palavra not in punctuation
                       and palavra.lower() != 'histórias'
                       and palavra.lower() != 'sobreviventes'
                       and palavra.lower() != 'suicídio'
                       and palavra.lower() != 'vol']

    return palavras_limpas


# Carregar o conteúdo do arquivo .docx
caminho_arquivo_docx = 'egoista.docx'
texto_docx = carregar_docx(caminho_arquivo_docx)

# Limpar o texto removendo stopwords, pontuação, aspas, pontos, traços, travessões e palavras específicas
palavras_limpas = limpar_texto(texto_docx)

# Carregar o dicionário SentiLex
sentilex = carregar_sentilex()

# Filtrar palavras que estão no SentiLex e calcular suas frequências
palavras_sentimentos = [palavra for palavra in palavras_limpas if palavra in sentilex]
frequencia_palavras_sentimentos = Counter(palavras_sentimentos)

# Selecionar as palavras com frequência maior que 2
top_palavras_filtradas = [(palavra, freq) for palavra, freq in frequencia_palavras_sentimentos.items() if freq > 2]

# Separar as palavras e suas frequências filtradas
palavras_filtradas, frequencias_filtradas = zip(*top_palavras_filtradas)

# Plotar o gráfico de barras das palavras de sentimentos filtradas
plt.figure(figsize=(14, 8))  # Ajustar o tamanho da figura conforme necessário
bars = plt.bar(palavras_filtradas, frequencias_filtradas, color='b', width=0.5)  # Ajustar a largura das barras conforme necessário
plt.title('Gráfico de Barras das Palavras de Sentimentos com Frequência Maior que 2', fontsize=16, fontweight='bold')
plt.xlabel('Palavra', fontsize=16, fontweight='bold')
plt.ylabel('Frequência', fontsize=16, fontweight='bold')
plt.xticks(rotation=90, fontsize=12, fontweight='bold', ha='right')  # Ajustar o ângulo e o tamanho da fonte no eixo x
# Adicionar valor de frequência em cima de cada barra
for bar, freq in zip(bars, frequencias_filtradas):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), str(freq), ha='center', va='bottom', fontsize=10, fontweight='bold')

"""# Frequencia das palavras chave (Gráfico de pareto com porcentagem acumulada)_Anomico"""

import matplotlib.pyplot as plt
from docx import Document
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from string import punctuation
from sklearn.feature_extraction.text import TfidfVectorizer

# Função para carregar o conteúdo de um arquivo .docx
def carregar_docx(caminho_arquivo):
    doc = Document(caminho_arquivo)
    texto = []
    for paragraph in doc.paragraphs:
        texto.append(paragraph.text)
    return '\n'.join(texto)

# Função para limpar o texto removendo stopwords, pontuação e palavras específicas
def limpar_texto(texto):
    # Tokenizar o texto em palavras
    palavras = word_tokenize(texto)

    # Obter lista de stopwords em português
    stopwords_portugues = set(stopwords.words('portuguese'))

    # Remover stopwords, pontuação e palavras específicas
    palavras_limpas = [palavra.lower() for palavra in palavras
                       if palavra.lower() not in stopwords_portugues
                       and palavra.lower() not in punctuation
                       and palavra.lower() != 'histórias'
                       and palavra.lower() != 'sobreviventes'
                       and palavra.lower() != 'suicídio'
                       and palavra.lower() != 'vol']

    # Juntar as palavras novamente em um texto limpo
    texto_limpo = ' '.join(palavras_limpas)

    return texto_limpo

# Carregar o conteúdo do arquivo .docx
caminho_arquivo_docx = 'Anômico.docx'
texto_docx = carregar_docx(caminho_arquivo_docx)

# Limpar o texto removendo stopwords, pontuação e palavras específicas
texto_limpo = limpar_texto(texto_docx)

# Inicializar e ajustar o modelo TF-IDF
tfidf_modelo = TfidfVectorizer()
tfidf_modelo.fit([texto_limpo])

# Obter os termos (palavras) e seus respectivos TF-IDF
termos = tfidf_modelo.get_feature_names_out()
tfidf_valores = tfidf_modelo.transform([texto_limpo]).toarray()[0]

# Definir um limite para o TF-IDF
limite_tfidf = 0.1

# Selecionar palavras com TF-IDF acima do limite
palavras_importantes_tfidf = [(termo, tfidf) for termo, tfidf in zip(termos, tfidf_valores) if tfidf > limite_tfidf]

# Separar as palavras e seus valores de TF-IDF
palavras_tfidf, valores_tfidf = zip(*palavras_importantes_tfidf) if palavras_importantes_tfidf else ([], [])




# Plotar o gráfico de linha das palavras consideradas importantes pelo TF-IDF
plt.figure(figsize=(10, 6))
plt.plot(palavras_tfidf, valores_tfidf, marker='o', linestyle='-', color='b')
plt.title('Anômico.docx Palavras Mais Importantes (TF-IDF > 0.1)', fontsize=16, fontweight='bold')
plt.xlabel('Palavra', fontsize=16, fontweight='bold')
plt.ylabel('TF-IDF', fontsize=16, fontweight='bold')
plt.xticks(rotation=45, fontsize=14, fontweight='bold', ha='right')
plt.grid(True)
plt.tight_layout()

# Adicionar rótulos com os números dos pontos
for i, txt in enumerate(valores_tfidf):
    plt.text(palavras_tfidf[i], txt, f'{txt:.2f}', fontsize=12, ha='center', va='bottom')

plt.show()



import matplotlib.pyplot as plt
from collections import Counter
from docx import Document
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from string import punctuation
import nltk
import re
import os

# Certifique-se de baixar os recursos necessários do NLTK
nltk.download('punkt')
nltk.download('stopwords')

# Função para carregar SentiLex-PT02
def carregar_sentilex():
    sentilex = {}
    caminho_sentilex = 'SentiLex-lem-PT02.txt'

    if not os.path.exists(caminho_sentilex):
        raise FileNotFoundError(f"O arquivo {caminho_sentilex} não foi encontrado.")

    with open(caminho_sentilex, 'r', encoding='utf-8') as f:
        for line in f:
            parts = line.strip().split(';')
            termo = parts[0].split('.')[0]  # Extrair a palavra
            polaridades = [int(re.search(r'POL:N\d=(-?\d)', part).group(1)) for part in parts if 'POL:N' in part]
            if polaridades:
                sentilex[termo] = polaridades[0]  # Usar a primeira polaridade encontrada
    return sentilex

# Função para carregar o conteúdo de um arquivo .docx
def carregar_docx(caminho_arquivo):
    doc = Document(caminho_arquivo)
    texto = []
    for paragraph in doc.paragraphs:
        texto.append(paragraph.text)
    return '\n'.join(texto)

# Função para limpar o texto removendo stopwords, pontuação, aspas, pontos, traços, travessões e palavras específicas
def limpar_texto(texto):
    # Remover pontos, traços, travessões e aspas
    texto = re.sub(r'[.–-“”]', ' ', texto)

    # Tokenizar o texto em palavras
    palavras = word_tokenize(texto)

    # Obter lista de stopwords em português
    stopwords_portugues = set(stopwords.words('portuguese'))

    # Remover stopwords, pontuação e palavras específicas
    palavras_limpas = [palavra.lower() for palavra in palavras
                       if palavra.lower() not in stopwords_portugues
                       and palavra not in punctuation
                       and palavra.lower() != 'histórias'
                       and palavra.lower() != 'sobreviventes'
                       and palavra.lower() != 'suicídio'
                       and palavra.lower() != 'vol']

    return palavras_limpas


# Carregar o conteúdo do arquivo .docx
caminho_arquivo_docx = 'Anômico.docx'
texto_docx = carregar_docx(caminho_arquivo_docx)

# Limpar o texto removendo stopwords, pontuação, aspas, pontos, traços, travessões e palavras específicas
palavras_limpas = limpar_texto(texto_docx)

# Carregar o dicionário SentiLex
sentilex = carregar_sentilex()

# Filtrar palavras que estão no SentiLex e calcular suas frequências
palavras_sentimentos = [palavra for palavra in palavras_limpas if palavra in sentilex]
frequencia_palavras_sentimentos = Counter(palavras_sentimentos)

# Selecionar as palavras com frequência maior que 2
top_palavras_filtradas = [(palavra, freq) for palavra, freq in frequencia_palavras_sentimentos.items() if freq > 3]

# Separar as palavras e suas frequências filtradas
palavras_filtradas, frequencias_filtradas = zip(*top_palavras_filtradas)

# Plotar o gráfico de barras das palavras de sentimentos filtradas
plt.figure(figsize=(14, 8))  # Ajustar o tamanho da figura conforme necessário
bars = plt.bar(palavras_filtradas, frequencias_filtradas, color='b', width=0.5)  # Ajustar a largura das barras conforme necessário
plt.title('Gráfico de Barras das Palavras de Sentimentos com Frequência Maior que 3', fontsize=16, fontweight='bold')
plt.xlabel('Palavra', fontsize=16, fontweight='bold')
plt.ylabel('Frequência', fontsize=16, fontweight='bold')
plt.xticks(rotation=90, fontsize=12, fontweight='bold', ha='right')  # Ajustar o ângulo e o tamanho da fonte no eixo x
# Adicionar valor de frequência em cima de cada barra
for bar, freq in zip(bars, frequencias_filtradas):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), str(freq), ha='center', va='bottom', fontsize=12, fontweight='bold')

"""# Frequencia das palavras chave (Gráfico de pareto com porcentagem acumulada)_relato_de_luto"""

import matplotlib.pyplot as plt
from docx import Document
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from string import punctuation
from sklearn.feature_extraction.text import TfidfVectorizer

# Função para carregar o conteúdo de um arquivo .docx
def carregar_docx(caminho_arquivo):
    doc = Document(caminho_arquivo)
    texto = []
    for paragraph in doc.paragraphs:
        texto.append(paragraph.text)
    return '\n'.join(texto)

# Função para limpar o texto removendo stopwords, pontuação e palavras específicas
def limpar_texto(texto):
    # Tokenizar o texto em palavras
    palavras = word_tokenize(texto)

    # Obter lista de stopwords em português
    stopwords_portugues = set(stopwords.words('portuguese'))

    # Remover stopwords, pontuação e palavras específicas
    palavras_limpas = [palavra.lower() for palavra in palavras
                       if palavra.lower() not in stopwords_portugues
                       and palavra.lower() not in punctuation
                       and palavra.lower() != 'histórias'
                       and palavra.lower() != 'sobreviventes'
                       and palavra.lower() != 'suicídio'
                       and palavra.lower() != 'vol']

    # Juntar as palavras novamente em um texto limpo
    texto_limpo = ' '.join(palavras_limpas)

    return texto_limpo

# Carregar o conteúdo do arquivo .docx
caminho_arquivo_docx = 'relato_de_luto.docx'
texto_docx = carregar_docx(caminho_arquivo_docx)

# Limpar o texto removendo stopwords, pontuação e palavras específicas
texto_limpo = limpar_texto(texto_docx)

# Inicializar e ajustar o modelo TF-IDF
tfidf_modelo = TfidfVectorizer()
tfidf_modelo.fit([texto_limpo])

# Obter os termos (palavras) e seus respectivos TF-IDF
termos = tfidf_modelo.get_feature_names_out()
tfidf_valores = tfidf_modelo.transform([texto_limpo]).toarray()[0]

# Definir um limite para o TF-IDF
limite_tfidf = 0.1

# Selecionar palavras com TF-IDF acima do limite
palavras_importantes_tfidf = [(termo, tfidf) for termo, tfidf in zip(termos, tfidf_valores) if tfidf > limite_tfidf]

# Separar as palavras e seus valores de TF-IDF
palavras_tfidf, valores_tfidf = zip(*palavras_importantes_tfidf) if palavras_importantes_tfidf else ([], [])


# Plotar o gráfico de linha das palavras consideradas importantes pelo TF-IDF
plt.figure(figsize=(10, 6))
plt.plot(palavras_tfidf, valores_tfidf, marker='o', linestyle='-', color='b')
plt.title('relato_de_luto.docx Palavras Mais Importantes (TF-IDF > 0.1)', fontsize=16, fontweight='bold')
plt.xlabel('Palavra', fontsize=16, fontweight='bold')
plt.ylabel('TF-IDF', fontsize=16, fontweight='bold')
plt.xticks(rotation=45, fontsize=14, fontweight='bold', ha='right')
plt.grid(True)
plt.tight_layout()

# Adicionar rótulos com os números dos pontos
for i, txt in enumerate(valores_tfidf):
    plt.text(palavras_tfidf[i], txt, f'{txt:.2f}', fontsize=12, ha='center', va='bottom')

plt.show()



import matplotlib.pyplot as plt
from collections import Counter
from docx import Document
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from string import punctuation
import nltk
import re
import os

# Certifique-se de baixar os recursos necessários do NLTK
nltk.download('punkt')
nltk.download('stopwords')

# Função para carregar SentiLex-PT02
def carregar_sentilex():
    sentilex = {}
    caminho_sentilex = 'SentiLex-lem-PT02.txt'

    if not os.path.exists(caminho_sentilex):
        raise FileNotFoundError(f"O arquivo {caminho_sentilex} não foi encontrado.")

    with open(caminho_sentilex, 'r', encoding='utf-8') as f:
        for line in f:
            parts = line.strip().split(';')
            termo = parts[0].split('.')[0]  # Extrair a palavra
            polaridades = [int(re.search(r'POL:N\d=(-?\d)', part).group(1)) for part in parts if 'POL:N' in part]
            if polaridades:
                sentilex[termo] = polaridades[0]  # Usar a primeira polaridade encontrada
    return sentilex

# Função para carregar o conteúdo de um arquivo .docx
def carregar_docx(caminho_arquivo):
    doc = Document(caminho_arquivo)
    texto = []
    for paragraph in doc.paragraphs:
        texto.append(paragraph.text)
    return '\n'.join(texto)

# Função para limpar o texto removendo stopwords, pontuação, aspas, pontos, traços, travessões e palavras específicas
def limpar_texto(texto):
    # Remover pontos, traços, travessões e aspas
    texto = re.sub(r'[.–-“”]', ' ', texto)

    # Tokenizar o texto em palavras
    palavras = word_tokenize(texto)

    # Obter lista de stopwords em português
    stopwords_portugues = set(stopwords.words('portuguese'))

    # Remover stopwords, pontuação e palavras específicas
    palavras_limpas = [palavra.lower() for palavra in palavras
                       if palavra.lower() not in stopwords_portugues
                       and palavra not in punctuation
                       and palavra.lower() != 'histórias'
                       and palavra.lower() != 'sobreviventes'
                       and palavra.lower() != 'suicídio'
                       and palavra.lower() != 'vol']

    return palavras_limpas


# Carregar o conteúdo do arquivo .docx
caminho_arquivo_docx = 'relato_de_luto.docx'
texto_docx = carregar_docx(caminho_arquivo_docx)

# Limpar o texto removendo stopwords, pontuação, aspas, pontos, traços, travessões e palavras específicas
palavras_limpas = limpar_texto(texto_docx)

# Carregar o dicionário SentiLex
sentilex = carregar_sentilex()

# Filtrar palavras que estão no SentiLex e calcular suas frequências
palavras_sentimentos = [palavra for palavra in palavras_limpas if palavra in sentilex]
frequencia_palavras_sentimentos = Counter(palavras_sentimentos)

# Selecionar as palavras com frequência maior que 2
top_palavras_filtradas = [(palavra, freq) for palavra, freq in frequencia_palavras_sentimentos.items() if freq > 3]

# Separar as palavras e suas frequências filtradas
palavras_filtradas, frequencias_filtradas = zip(*top_palavras_filtradas)

# Plotar o gráfico de barras das palavras de sentimentos filtradas
plt.figure(figsize=(14, 8))  # Ajustar o tamanho da figura conforme necessário
bars = plt.bar(palavras_filtradas, frequencias_filtradas, color='b', width=0.5)  # Ajustar a largura das barras conforme necessário
plt.title('Gráfico de Barras das Palavras de Sentimentos com Frequência Maior que 3', fontsize=16, fontweight='bold')
plt.xlabel('Palavra', fontsize=16, fontweight='bold')
plt.ylabel('Frequência', fontsize=16, fontweight='bold')
plt.xticks(rotation=90, fontsize=12, fontweight='bold', ha='right')  # Ajustar o ângulo e o tamanho da fonte no eixo x
# Adicionar valor de frequência em cima de cada barra
for bar, freq in zip(bars, frequencias_filtradas):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), str(freq), ha='center', va='bottom', fontsize=12, fontweight='bold')

"""# Frequencia das palavras chave (Gráfico de pareto com porcentagem acumulada)_fatalista"""

import matplotlib.pyplot as plt
from docx import Document
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from string import punctuation
from sklearn.feature_extraction.text import TfidfVectorizer

# Função para carregar o conteúdo de um arquivo .docx
def carregar_docx(caminho_arquivo):
    doc = Document(caminho_arquivo)
    texto = []
    for paragraph in doc.paragraphs:
        texto.append(paragraph.text)
    return '\n'.join(texto)

# Função para limpar o texto removendo stopwords, pontuação e palavras específicas
def limpar_texto(texto):
    # Tokenizar o texto em palavras
    palavras = word_tokenize(texto)

    # Obter lista de stopwords em português
    stopwords_portugues = set(stopwords.words('portuguese'))

    # Remover stopwords, pontuação e palavras específicas
    palavras_limpas = [palavra.lower() for palavra in palavras
                       if palavra.lower() not in stopwords_portugues
                       and palavra.lower() not in punctuation
                       and palavra.lower() != 'histórias'
                       and palavra.lower() != 'sobreviventes'
                       and palavra.lower() != 'suicídio'
                       and palavra.lower() != 'vol']

    # Juntar as palavras novamente em um texto limpo
    texto_limpo = ' '.join(palavras_limpas)

    return texto_limpo

# Carregar o conteúdo do arquivo .docx
caminho_arquivo_docx = 'fatalista.docx'
texto_docx = carregar_docx(caminho_arquivo_docx)

# Limpar o texto removendo stopwords, pontuação e palavras específicas
texto_limpo = limpar_texto(texto_docx)

# Inicializar e ajustar o modelo TF-IDF
tfidf_modelo = TfidfVectorizer()
tfidf_modelo.fit([texto_limpo])

# Obter os termos (palavras) e seus respectivos TF-IDF
termos = tfidf_modelo.get_feature_names_out()
tfidf_valores = tfidf_modelo.transform([texto_limpo]).toarray()[0]

# Definir um limite para o TF-IDF
limite_tfidf = 0.1

# Selecionar palavras com TF-IDF acima do limite
palavras_importantes_tfidf = [(termo, tfidf) for termo, tfidf in zip(termos, tfidf_valores) if tfidf > limite_tfidf]

# Separar as palavras e seus valores de TF-IDF
palavras_tfidf, valores_tfidf = zip(*palavras_importantes_tfidf) if palavras_importantes_tfidf else ([], [])


# Plotar o gráfico de linha das palavras consideradas importantes pelo TF-IDF
plt.figure(figsize=(10, 6))
plt.plot(palavras_tfidf, valores_tfidf, marker='o', linestyle='-', color='b')
plt.title('fatalista.docx Palavras Mais Importantes (TF-IDF > 0.1)', fontsize=16, fontweight='bold')
plt.xlabel('Palavra', fontsize=16, fontweight='bold')
plt.ylabel('TF-IDF', fontsize=16, fontweight='bold')
plt.xticks(rotation=45, fontsize=14, fontweight='bold', ha='right')
plt.grid(True)
plt.tight_layout()

# Adicionar rótulos com os números dos pontos
for i, txt in enumerate(valores_tfidf):
    plt.text(palavras_tfidf[i], txt, f'{txt:.2f}', fontsize=12, ha='center', va='bottom')

plt.show()



import matplotlib.pyplot as plt
from collections import Counter
from docx import Document
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from string import punctuation
import nltk
import re
import os

# Certifique-se de baixar os recursos necessários do NLTK
nltk.download('punkt')
nltk.download('stopwords')

# Função para carregar SentiLex-PT02
def carregar_sentilex():
    sentilex = {}
    caminho_sentilex = 'SentiLex-lem-PT02.txt'

    if not os.path.exists(caminho_sentilex):
        raise FileNotFoundError(f"O arquivo {caminho_sentilex} não foi encontrado.")

    with open(caminho_sentilex, 'r', encoding='utf-8') as f:
        for line in f:
            parts = line.strip().split(';')
            termo = parts[0].split('.')[0]  # Extrair a palavra
            polaridades = [int(re.search(r'POL:N\d=(-?\d)', part).group(1)) for part in parts if 'POL:N' in part]
            if polaridades:
                sentilex[termo] = polaridades[0]  # Usar a primeira polaridade encontrada
    return sentilex

# Função para carregar o conteúdo de um arquivo .docx
def carregar_docx(caminho_arquivo):
    doc = Document(caminho_arquivo)
    texto = []
    for paragraph in doc.paragraphs:
        texto.append(paragraph.text)
    return '\n'.join(texto)

# Função para limpar o texto removendo stopwords, pontuação, aspas, pontos, traços, travessões e palavras específicas
def limpar_texto(texto):
    # Remover pontos, traços, travessões e aspas
    texto = re.sub(r'[.–-“”]', ' ', texto)

    # Tokenizar o texto em palavras
    palavras = word_tokenize(texto)

    # Obter lista de stopwords em português
    stopwords_portugues = set(stopwords.words('portuguese'))

    # Remover stopwords, pontuação e palavras específicas
    palavras_limpas = [palavra.lower() for palavra in palavras
                       if palavra.lower() not in stopwords_portugues
                       and palavra not in punctuation
                       and palavra.lower() != 'histórias'
                       and palavra.lower() != 'sobreviventes'
                       and palavra.lower() != 'suicídio'
                       and palavra.lower() != 'vol']

    return palavras_limpas


# Carregar o conteúdo do arquivo .docx
caminho_arquivo_docx = 'fatalista.docx'
texto_docx = carregar_docx(caminho_arquivo_docx)

# Limpar o texto removendo stopwords, pontuação, aspas, pontos, traços, travessões e palavras específicas
palavras_limpas = limpar_texto(texto_docx)

# Carregar o dicionário SentiLex
sentilex = carregar_sentilex()

# Filtrar palavras que estão no SentiLex e calcular suas frequências
palavras_sentimentos = [palavra for palavra in palavras_limpas if palavra in sentilex]
frequencia_palavras_sentimentos = Counter(palavras_sentimentos)

# Selecionar as palavras com frequência maior que 2
top_palavras_filtradas = [(palavra, freq) for palavra, freq in frequencia_palavras_sentimentos.items() if freq > 3]

# Separar as palavras e suas frequências filtradas
palavras_filtradas, frequencias_filtradas = zip(*top_palavras_filtradas)

# Plotar o gráfico de barras das palavras de sentimentos filtradas
plt.figure(figsize=(14, 8))  # Ajustar o tamanho da figura conforme necessário
bars = plt.bar(palavras_filtradas, frequencias_filtradas, color='b', width=0.5)  # Ajustar a largura das barras conforme necessário
plt.title('Gráfico de Barras das Palavras de Sentimentos com Frequência Maior que 3', fontsize=16, fontweight='bold')
plt.xlabel('Palavra', fontsize=16, fontweight='bold')
plt.ylabel('Frequência', fontsize=16, fontweight='bold')
plt.xticks(rotation=90, fontsize=12, fontweight='bold', ha='right')  # Ajustar o ângulo e o tamanho da fonte no eixo x
# Adicionar valor de frequência em cima de cada barra
for bar, freq in zip(bars, frequencias_filtradas):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), str(freq), ha='center', va='bottom', fontsize=12, fontweight='bold')